{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_3_Autoencoder_Dimensionality_Reduction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GTY-Msjal7zU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Tutorial: Autoencoder for Dimensionality Reduction\n",
        "\n",
        "### Author: Ivan Bongiorni, Data Scientist at GfK.\n",
        "\n",
        "[LinkedIn profile](https://www.linkedin.com/in/ivan-bongiorni-b8a583164/), personal email: ivanbongiorni@gmail.com\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Z3M5ezRyrjA3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0. Summary\n",
        "\n",
        "\n",
        "\n",
        "1.   What are Autoencoders?\n",
        "2.   Why would you need Autoencoders?\n",
        "3.   Implementation\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Kg83oWxMm92f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. What are Autoencoders?\n",
        "\n",
        "This tutorial is about a very specific form of Neural Networks: **Autoencoders**.  \n",
        "\n",
        "There are several kinds of Autoencoders around, such as denoising Autoencoders, and generative models (variational Autoencoders and GANs); they are all meant to accopmlish very specific and different tasks. In this tutotrial, I'll focus on probably the simplest of all: dimensionality reduction. Autoencoders for dimensionality reduction can be conceived as the Neural Network-equivalent of Principal Component Analysis.\n",
        "\n",
        "The purpose of this class fo models is neither to learn how to classify observations, nor to predict a continuous output, but to compress, or simplify a dataset. For this reason, Autoencoders for dimensionality reduction are unseupervised models, instead of mainstream Deep Learning models. All the data that pass through an Autoencoder are unlabeled.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IpcaQoQ9srG6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Structure\n",
        "\n",
        "If I had to explain an Autoencoder to my grand mother, I'd do it like this: it can be seen as a couple of funnels joined by the tip: in order to make data must flow from one extreme to the other, you must force them through the bottleneck in the middle.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AvZTU-HzmZEo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "More formally, it can be described as follows:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)\n",
        "\n",
        "(image from [towardsdatascience](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798))"
      ]
    },
    {
      "metadata": {
        "id": "cAxMjUCBUa43",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is composed of two parts, an **Encoder** and a **Decoder**. The goal of the Encoder is to \"compress\" the dataset, representing its features with a number of nodes/variables that is narrower than the output layer; the goal of the Decoder is to learn how to reproduce, from the central encoded layer, the initial input values. Autoencoder are thus meant to reproduce the input data on the other extreme as accurately as possible.\n",
        "\n",
        "At this point, you have the right to be puzzled. The first time I've read about this architecture, I was like \"Wait a minute, what's the point of all that?!\". And in fact, it's completely pointless. But reproducing the same dataset on the other side of the Network is a trick that allows for a dimensionality reduction.\n",
        "\n",
        "The fact that the central hidden layers is narrower than the extremes forces the network to find the most efficient way to represent, at that layer, the same data in a more compressed form. The data that flow through that central layer can be seen as a form of dimensionality reduction."
      ]
    },
    {
      "metadata": {
        "id": "KMWIJUSerRVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Why would you need Autoencoders?\n",
        "\n",
        "Why would you need dimensionality reduction?\n",
        "\n",
        "As you have probably learned from PCA, dimensionality reduction techniques are particularly useful when dealing with very complex and/or multicollinear datasets. Thanks to the \"simplification\" they perform, it is also possible to train Machine Learnign models on datasets otherwise too big for your run-down old laptop (joking).\n",
        "\n",
        "### Ok, but why not good old PCA?\n",
        "\n",
        "The advantages of this kind of Neural Network architecture over the more classical PCA is that with an Autoencoder you can potentially capture any non-linear pattern of your data. From PCA instead, you can extract factors that are only linearly associated with your actual variables, making the process of dimensionality reduction much more \"rigid\"."
      ]
    },
    {
      "metadata": {
        "id": "eqK0oiaTsAQn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Implementation\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-3rPWJBemIEb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import data from UCI ML repository\n",
        "\n",
        "The purpose of this dataset is to reduce the dimensionality of the breast cancer dataset. cases between **malignant** (**M**), and **benign** (**B**). Therefore, my classification network will have two output nodes."
      ]
    },
    {
      "metadata": {
        "id": "kOKrNBGSlz2t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", sep=\",\", header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HTXgNVxD2VYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataprep\n",
        "\n",
        "Data preprocessing for Autoencoder require less effort than canonical Neural Networks. First, there is no need to split the data in train and test set, since my goal is just to produce a compressed version of the same dataset. Second, data do not require any labeling; we are in the field of unsupervised Machine Learning."
      ]
    },
    {
      "metadata": {
        "id": "anj7e1keqC-K",
        "colab_type": "code",
        "outputId": "bec2f761-bdbb-4a5f-9538-5c78de0052ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0  1      2      3       4       5        6        7       8   \\\n",
              "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
              "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
              "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
              "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
              "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
              "\n",
              "        9    ...        22     23      24      25      26      27      28  \\\n",
              "0  0.14710   ...     25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119   \n",
              "1  0.07017   ...     24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416   \n",
              "2  0.12790   ...     23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504   \n",
              "3  0.10520   ...     14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869   \n",
              "4  0.10430   ...     22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000   \n",
              "\n",
              "       29      30       31  \n",
              "0  0.2654  0.4601  0.11890  \n",
              "1  0.1860  0.2750  0.08902  \n",
              "2  0.2430  0.3613  0.08758  \n",
              "3  0.2575  0.6638  0.17300  \n",
              "4  0.1625  0.2364  0.07678  \n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "R1NPnVJRbUbe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I don't need the first two columns:\n",
        "# the first is an id, the other is the target variable ('M'/'B')\n",
        "df = df.iloc[:,2:]\n",
        "\n",
        "# Turn the dataframe into a numpy object\n",
        "df = df.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8MZswZsz2POl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize the data - this time there is no need to use sklearn's StandardScaler() but you can use it if you want\n",
        "def normalize(x):\n",
        "    normalized = (x - np.mean(x))/(np.std(x))\n",
        "    return normalized\n",
        "\n",
        "for i in range(df.shape[1]):\n",
        "    df[:,i] = normalize(df[:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ibv0OMUj7sdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Architecture\n",
        "\n",
        "Given the shape of my dataframe:"
      ]
    },
    {
      "metadata": {
        "id": "Ac14zUWt-tGG",
        "colab_type": "code",
        "outputId": "008f1378-74d7-40c0-b6a5-dc6342fcd757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "J-cA8x_D-6bo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I must now choose an Autoencoder architecture. I want to compress a `(569, 30)` shaped dataset to a `(569, 10)` encoded version. This is an arbitrary choice. In real datascience problems the chioce depends from the nature of your specific dataset (and your specific needs), there is no general rule to determine the right number of encoded variables to be obtained.\n"
      ]
    },
    {
      "metadata": {
        "id": "qus4X_4XH0V8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I choose the size of each layer\n",
        "\n",
        "n_input_layer = df.shape[1]\n",
        "n_hidden1 = 30\n",
        "n_hidden2 = 15\n",
        "n_hidden3 = 15\n",
        "\n",
        "n_encoding_layer = 10\n",
        "\n",
        "n_hidden5 = 15\n",
        "n_hidden6 = 15\n",
        "n_hidden7 = 30\n",
        "n_output_layer = n_input_layer  # of course, the output layer must be of the same size of the input layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITiY0308HcuD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the architecture of the Network si perfectly symmetric. Of course, Encoder and Decoder don't need to be the exact mirror image of each other.\n",
        "\n",
        "First, I must create *one* placeholder to feed the data in:"
      ]
    },
    {
      "metadata": {
        "id": "aX8aKmFF7k4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Placeholders are kind of \"empty variables\" in a TF computational graph.\n",
        "# I will feed the actual data through them - in the graph, they are like entry doors for my data\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape = [None, df.shape[1]], name='X')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kF4aiNkw8cYq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This time we don't need placeholder y, since prediction and classification are off topic here. Our model doesn't require any dependent variable to be explained."
      ]
    },
    {
      "metadata": {
        "id": "kcOyYg3Q8OZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import fully_connected  # import the layers' function\n",
        "\n",
        "\n",
        "\n",
        "Input_layer = fully_connected(X, n_input_layer, activation_fn=tf.nn.leaky_relu, scope=\"input_layer\")\n",
        "\n",
        "hidden1 = fully_connected(Input_layer, n_hidden1, activation_fn=tf.nn.leaky_relu, scope=\"hidden_layer1\")\n",
        "\n",
        "hidden2 = fully_connected(hidden1, n_hidden2, activation_fn=tf.nn.leaky_relu, scope=\"hidden_layer2\")\n",
        "\n",
        "hidden3 = fully_connected(hidden2, n_hidden3, activation_fn=tf.nn.leaky_relu, scope=\"hidden_layer3\")\n",
        "\n",
        "\n",
        "# This is the central layer of the Autoencoder\n",
        "Encoding_layer = fully_connected(hidden3, n_encoding_layer, activation_fn=tf.nn.leaky_relu, scope=\"encoding_layer\")\n",
        "\n",
        "\n",
        "hidden5 = fully_connected(Encoding_layer, n_hidden5, activation_fn=tf.nn.leaky_relu, scope=\"hidden_layer5\")\n",
        "\n",
        "hidden6 = fully_connected(hidden5, n_hidden6, activation_fn=tf.nn.leaky_relu, scope=\"hidden_layer6\")\n",
        "\n",
        "hidden7 = fully_connected(hidden6, n_hidden7, activation_fn=tf.nn.leaky_relu, scope=\"hidden_layer7\")\n",
        "\n",
        "Output_layer = fully_connected(hidden7, n_output_layer, activation_fn=tf.nn.leaky_relu, scope=\"output_layer\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HrgXlzQMWzek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that the Network's structure is ready, let me define other hyperparameters:"
      ]
    },
    {
      "metadata": {
        "id": "X6iFujQiXZwR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set number of epochs\n",
        "n_epochs = 5000\n",
        "\n",
        "# set learning rate\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xMVYavgX3x6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The implementation of the loss function deserves attention.\n",
        "\n",
        "As I already explained above, the implementation of an Autoencoder doesn't require a dependent variable (nor its specific placeholder). For this reason, our loss function will not contain any \"y\". We only need to train the Autoencoder to behave as such, that is, to replicate the input data at the output layer. It's almost like comparing the initial dataset with a copy of itself.\n"
      ]
    },
    {
      "metadata": {
        "id": "LBq-IZ7eXr-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# implement Mean Squared Error\n",
        "loss = tf.reduce_mean(tf.square(X - Output_layer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q_PcewQSapyD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the TensorFlow operation that trains the model\n",
        "training_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KiGUWCol-uG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "4kuZB-jWmNuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()   # set a saver for saving the model at checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oevz4mHWmCKs",
        "colab_type": "code",
        "outputId": "0e0cedfe-d45f-4a1d-cc35-78086e7efe47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 968
        }
      },
      "cell_type": "code",
      "source": [
        "loss_history = []    # I want to monitor the loss through time\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())   # turns on the machine\n",
        "    \n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        sess.run(training_op, feed_dict={X: df})    # we have only X to feed, that is the dataset that we want to compress.\n",
        "        \n",
        "        current_loss = sess.run(loss, feed_dict={X: df})  # store the loss at each iteration\n",
        "        loss_history.append(current_loss)\n",
        "        \n",
        "        # Print the status of our Gradient Descent every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            print(str(epoch+1) + \". Loss: \" + str(current_loss) + \",\")\n",
        "    \n",
        "    # At the end of the training, you can save the model if you want\n",
        "    save_path = saver.save(sess, \"./autoencoder_1.ckpt\")\n",
        "    \n",
        "    # In this case, I want to use it right away, therefore I immediately save the encoded version of my\n",
        "    # It is done by taking the compress data from the central layer\n",
        "    encoded_dataframe = Encoding_layer.eval(feed_dict={X: df})   # the .eval argument is another way of getting the value of a tensor at some node\n",
        "    \n",
        "    print()\n",
        "    print(\"Encoding complete.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Loss: 0.99262094,\n",
            "101. Loss: 0.41691995,\n",
            "201. Loss: 0.3260689,\n",
            "301. Loss: 0.27633584,\n",
            "401. Loss: 0.2104583,\n",
            "501. Loss: 0.17014599,\n",
            "601. Loss: 0.14291686,\n",
            "701. Loss: 0.12853236,\n",
            "801. Loss: 0.119778134,\n",
            "901. Loss: 0.110465385,\n",
            "1001. Loss: 0.0985637,\n",
            "1101. Loss: 0.090854734,\n",
            "1201. Loss: 0.086306006,\n",
            "1301. Loss: 0.08299174,\n",
            "1401. Loss: 0.08068939,\n",
            "1501. Loss: 0.07875693,\n",
            "1601. Loss: 0.07702099,\n",
            "1701. Loss: 0.07558731,\n",
            "1801. Loss: 0.074517354,\n",
            "1901. Loss: 0.07346491,\n",
            "2001. Loss: 0.07234934,\n",
            "2101. Loss: 0.07148471,\n",
            "2201. Loss: 0.07074828,\n",
            "2301. Loss: 0.06996434,\n",
            "2401. Loss: 0.06903634,\n",
            "2501. Loss: 0.068289794,\n",
            "2601. Loss: 0.067636795,\n",
            "2701. Loss: 0.06687024,\n",
            "2801. Loss: 0.065924644,\n",
            "2901. Loss: 0.065062016,\n",
            "3001. Loss: 0.06418239,\n",
            "3101. Loss: 0.06324721,\n",
            "3201. Loss: 0.06251593,\n",
            "3301. Loss: 0.061682336,\n",
            "3401. Loss: 0.060844626,\n",
            "3501. Loss: 0.06006871,\n",
            "3601. Loss: 0.059311606,\n",
            "3701. Loss: 0.05862759,\n",
            "3801. Loss: 0.0579947,\n",
            "3901. Loss: 0.057262428,\n",
            "4001. Loss: 0.056626443,\n",
            "4101. Loss: 0.05609803,\n",
            "4201. Loss: 0.0556319,\n",
            "4301. Loss: 0.055313542,\n",
            "4401. Loss: 0.054779306,\n",
            "4501. Loss: 0.05464718,\n",
            "4601. Loss: 0.053952657,\n",
            "4701. Loss: 0.053646643,\n",
            "4801. Loss: 0.05324493,\n",
            "4901. Loss: 0.052860137,\n",
            "\n",
            "Encoding complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2f1s2U2mpqK",
        "colab_type": "code",
        "outputId": "6c996b63-ca02-47f7-a2f5-32d034773300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(loss_history, label=\"MSE\")\n",
        "plt.title(\"Autoencoder's loss through the epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW/yqUveIGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mEDmoV4dfSD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Applications\n",
        "\n",
        "Now that we have an encoded version of our initial dataset, we can use it for our Machine Learning task.\n",
        "\n",
        "The encoded dataset is stored into a numpy array called `encoded_dataframe`, that we can turn into a pandas DataFrame:"
      ]
    },
    {
      "metadata": {
        "id": "xvUvSvQEgAV2",
        "colab_type": "code",
        "outputId": "263a8924-800c-4e58-d80c-17b15e347430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "cell_type": "code",
      "source": [
        "# You can save it in a pandas dataframe\n",
        "encoded_pandas_dataframe = pd.DataFrame(encoded_dataframe)\n",
        "\n",
        "# Let's take a look:\n",
        "encoded_pandas_dataframe.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.950812</td>\n",
              "      <td>16.487156</td>\n",
              "      <td>7.728285</td>\n",
              "      <td>3.261057</td>\n",
              "      <td>6.443383</td>\n",
              "      <td>5.501568</td>\n",
              "      <td>11.683473</td>\n",
              "      <td>3.548657</td>\n",
              "      <td>6.374877</td>\n",
              "      <td>10.508931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>6.478880</td>\n",
              "      <td>10.128191</td>\n",
              "      <td>4.617937</td>\n",
              "      <td>2.953408</td>\n",
              "      <td>6.955463</td>\n",
              "      <td>4.555366</td>\n",
              "      <td>8.673910</td>\n",
              "      <td>4.639213</td>\n",
              "      <td>4.217002</td>\n",
              "      <td>9.891843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.106495</td>\n",
              "      <td>0.565276</td>\n",
              "      <td>-0.185673</td>\n",
              "      <td>-1.971750</td>\n",
              "      <td>-0.214498</td>\n",
              "      <td>-0.894893</td>\n",
              "      <td>-1.230408</td>\n",
              "      <td>-5.819337</td>\n",
              "      <td>-0.267922</td>\n",
              "      <td>-2.185170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.837411</td>\n",
              "      <td>8.317730</td>\n",
              "      <td>4.729818</td>\n",
              "      <td>0.815832</td>\n",
              "      <td>1.876604</td>\n",
              "      <td>2.501072</td>\n",
              "      <td>4.820455</td>\n",
              "      <td>-0.615608</td>\n",
              "      <td>3.747824</td>\n",
              "      <td>-0.008073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.444600</td>\n",
              "      <td>14.563843</td>\n",
              "      <td>7.041510</td>\n",
              "      <td>2.875292</td>\n",
              "      <td>3.979146</td>\n",
              "      <td>4.546839</td>\n",
              "      <td>9.438953</td>\n",
              "      <td>3.218110</td>\n",
              "      <td>5.604097</td>\n",
              "      <td>10.139227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.072370</td>\n",
              "      <td>22.776043</td>\n",
              "      <td>9.779955</td>\n",
              "      <td>5.282134</td>\n",
              "      <td>8.848096</td>\n",
              "      <td>7.434040</td>\n",
              "      <td>17.366352</td>\n",
              "      <td>7.063730</td>\n",
              "      <td>8.010862</td>\n",
              "      <td>18.109919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>38.690048</td>\n",
              "      <td>53.922176</td>\n",
              "      <td>47.080498</td>\n",
              "      <td>15.525367</td>\n",
              "      <td>39.998653</td>\n",
              "      <td>40.229443</td>\n",
              "      <td>40.605808</td>\n",
              "      <td>18.224545</td>\n",
              "      <td>45.316860</td>\n",
              "      <td>40.703217</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0           1           2           3           4           5  \\\n",
              "count  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000   \n",
              "mean     5.950812   16.487156    7.728285    3.261057    6.443383    5.501568   \n",
              "std      6.478880   10.128191    4.617937    2.953408    6.955463    4.555366   \n",
              "min     -0.106495    0.565276   -0.185673   -1.971750   -0.214498   -0.894893   \n",
              "25%      1.837411    8.317730    4.729818    0.815832    1.876604    2.501072   \n",
              "50%      3.444600   14.563843    7.041510    2.875292    3.979146    4.546839   \n",
              "75%      8.072370   22.776043    9.779955    5.282134    8.848096    7.434040   \n",
              "max     38.690048   53.922176   47.080498   15.525367   39.998653   40.229443   \n",
              "\n",
              "                6           7           8           9  \n",
              "count  569.000000  569.000000  569.000000  569.000000  \n",
              "mean    11.683473    3.548657    6.374877   10.508931  \n",
              "std      8.673910    4.639213    4.217002    9.891843  \n",
              "min     -1.230408   -5.819337   -0.267922   -2.185170  \n",
              "25%      4.820455   -0.615608    3.747824   -0.008073  \n",
              "50%      9.438953    3.218110    5.604097   10.139227  \n",
              "75%     17.366352    7.063730    8.010862   18.109919  \n",
              "max     40.605808   18.224545   45.316860   40.703217  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "gFYL7l26fahk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this point, you can train any Machine Learning model on the encoded dataframe (for example, [some feed-forward Neural Network](https://github.com/IvanBongiorni/TensorFlow_Tutorial/blob/master/TensorFlow_1_Classification_BatchGD.ipynb), as I explained in previous tutorials, or [the Mini-Batch Gradient Descent version of it](https://github.com/IvanBongiorni/TensorFlow_Tutorial/blob/master/TensorFlow_2_Classification_MiniBatchGD.ipynb), why not).\n",
        "\n",
        "You'll find that computational times will be much slower, and with very similar results. In case of very multicollinear datasets, you might even find a significant improvement in your model performance.\n"
      ]
    }
  ]
}